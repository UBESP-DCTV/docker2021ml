---
title: "Text Mining -- Hands-on"
subtitle: "v-1.2.0"
author: "Dott.ssa Ileana Baldi -- Dott. Corrado Lanera"
output: 
  html_notebook: 
    toc: yes
---

# Outline
+ Preamble
    - strategy
    - packages
+ Execution
    - Loading
    - Preprocess
    - Analyses
    - Visualization
+ Links







# Preamble

In this Hands-on we will analyze a set of bibliographic citations
downloaded from PubMed <https://www.ncbi.nlm.nih.gov/pubmed>. Our aim
will be to extract some (sample) information to have the possibility to
inspect each step of the text analysis process: retrieval, cleaning,
transform, analyze.

We are interested to analyze all the paper published on PubMed related
to the field of Machine Leaning in the last ten years. So go to the
website, perform the following search:

    ("machine learning") AND ("2007/01/01"[Date - Publication] : "2017/01/01"[Date - Publication]) 

At February, 8th 2017 the search returned 9872 papers. Once we have the
results, we can download them in the format we prefer, today we choose
for the standard one: a Comma-Separated Value (`pubmed_result.csv`).

It is a good idea to start to inspect it with a simple text-editor to
have a first look at the possible issues.

At the firs look, I see only the presence of iterative header every 50
rows more or less.



## Strategy

+ (simulate the) data download
+ setup R with the useful packages
+ Loading
    - importing data
    - create a _corpus_
+ preprocess
    - lowering
    - stopwords
    - punctuation
    - numbers
    - whitespace
    - stemming
    - 2-Gram
    - DTM creation (lengths, bounds, weights)
+ analyses 
    - Frequencies
    - Association
+ Visualization
    - barplot
    - wordcloud
    - network



## Packages

There are some useful package I would like to introduce you, form the
general ones for data manipulation, to the ones more specifically
focused on text manipulation and management.


### Other

```{r other, warning=FALSE}
if (!requireNamespace('wordcloud', quietly = TRUE)) {
    install.packages('wordcloud', dependencies = TRUE)
}

# Create word clouds
library('wordcloud', quietly = TRUE)

citation('wordcloud')

#  -------

if (!requireNamespace('igraph', quietly = TRUE)) {
    install.packages('igraph', dependencies = TRUE)
}

# Simple graphs and network analysis
library('igraph', quietly = TRUE)

citation('igraph')
```


### String manipulation

```{r stringr, warning=FALSE}
if (!requireNamespace('stringr', quietly = TRUE)) {
    install.packages('stringr', dependencies = TRUE)
}

# A consistent, simple and easy to use set of wrappers around the
# fantastic 'stringi' package
library('stringr', quietly = TRUE)

citation('stringr')
```


### Text Manipulation

```{r tm, warning=FALSE}
if (!requireNamespace('tm', quietly = TRUE)) {
    install.packages('tm', dependencies = TRUE)
}

# Text Mining Package... the main focus of this hands-on 
library('tm', quietly = TRUE)

citation('tm')
```


### Data management (a single package bring-it-all)

```{r tidyverse, warning=FALSE}
if (!requireNamespace('tidyverse', quietly = TRUE)) {
    install.packages('tidyverse', dependencies = TRUE)
}

# Easily Install and Load 'Tidyverse' Packages...
# ...they share common data representations and 'API' design.
library('tidyverse', quietly = TRUE)

# ggplot2: grammar of graphics
# tibble : trimmed down version of data.frame
# tidyr  : Easily Tidy Data with 'spread()' and 'gather()' Functions
# readr  : Read flat/tabular text files from disk (or a connection)
# purrr  : Functional Programming Tools
# dplyr  : A fast, consistent tool for working with data frame like objects,
#          both in memory and out of memory

citation('tidyverse')

# NOTE   : the tidyverse packages import the _pipe_ operator (%>%) from 
#          the package `magrittr`. This little and simple operator is maybe
#          the most amazing tool you can learn to use. Its simplest works is
#          to let you avoid to write `f(x, y)` in favour of `x %>% f(y)`.
#          (Take a look at <http://r4ds.had.co.nz/pipes.html>)

```






# Execution



## Loading


### import

Start importing pubmed_result.csv into the workspace.

```{r error, error=TRUE}
original_file <- file.path(getwd(), 'pubmed_result.csv')

data <- read_csv(original_file)

## standard alternative
#
# data <- read.csv(
#     file            = original_file,    
#     stringsAsFactor = FALSE, 
#     header          = TRUE
#    )
```

There are some problems with the columns (i.e. separators), it seams
that the data have one column more than the expected ones (i.e. number of
headers). Take a look at it

```{r read_lines}
bad_text <- read_lines(original_file)

head(bad_text)
```

As expected there is a final extra "comma" at the end of each line.

Take now a look at the number of document (remember that we have retrieved 9872
document but we expect some fake row made of extra headers)

```{r length_bad}
length(bad_text)
```


Lets clean the file, first of all we have to remove all the header but
the first (remember that the header have no problem with the ending
commas). So we can remove all the headers, remove all the ending commas,
reinsert the header at the top (i.e. in the first position), merge all
the element in a single one textual file dividing the lines with a "new
line" character (i.e. `\n`) and finish the process by creating a data
frame with all the data.

(As alternative, you can write the corrected character vector as a csv
file into the hard-disk and next read it as before.)

Note: to clarify the use of the dot (".") with pipe read the
      corresponding (`?magrittr::\`%>%\``) documentation.
  
```{r good}
header <- bad_text[[1]]

good_data <- bad_text %>% 
    .[. != header] %>%                               # remove al the header rows
    str_replace_all(pattern = ",$", replacement = '') %>% # remove ending commas
    c(header, .) %>%                                  # reinsert the main header
    str_c(collapse = '\n') %>%            # create a single csv-like text vector
    read_csv()                                       # read it into a data frame

good_data
```

9872 elements as expected, no error, and no factor variables.



### corpus

To create the corpus of our document we have to retain only the `Title`.
Next, as it is said into the documentation of `tm::VCorpus` (`?VCorpus`)
we have to provide to this function a "Source" object (our object is a
data frame, so take a look at `?DataframeSource`). As we can see we need
a first column named "`doc_id`" which contains a unique string
identifier for each document, and a second column named "`text`" which
must contains a UTF-8 encoded string representing the document content.
Wecan use the url as the unique identifier.

Once create it we can produce our corpus.

```{r corpus}
# ?DataframeSource
# ?VCorpus

corpus <- good_data %>%
    dplyr::select(URL, Title) %>%
    dplyr::rename(doc_id = URL, text = Title) %>% 
    DataframeSource() %>% 
    VCorpus()

names(corpus) <- good_data$URL

corpus
```


To access to individual documents use `inspect()`.
```{r}
set.seed(1234567890)
watch <-  sample(length(corpus), 5)
inspect(corpus[watch])
```

Seven metadata each and a content each document. Take a look at them.
```{r}
# classical
str(corpus[[1]])

# specific
content(corpus[[1]])
meta(corpus[[1]])
meta(corpus[[1]], 'author')

# tm:::meta.PlainTextDocument
# tm:::content.PlainTextDocument
# tm:::inspect.VCorpus
```


<!-- OPTIONAL (triky) --------------------------------------------------------->
Lets set some meta data at document level. To do this we can access at the
structure of the corpus 

```{r}
names(good_data)

# behind the scene a VCorps is a list of 3 element (take a look at what is
# returned by the function `VCorpus()` (run `VCorpus` and look at the last
# line), and what it is: run `tm:::as.VCorpus.list`). So, we want to modify its
# `content`. map2 scan two list in parallel (`.x` and `.y`) applying the defined
# function (at the right of the `~`) to each corresponding couple of elements.
# The content of a VCorpus is a list of document, and each one of them is a 
# list of two: its content and its meta (take a look at `DataframeSource` 
# which call `SimpleSource()` to define the reader which, by default, is
# `readPlain` which call `PlainTextDocument()` returning that list). So,
# we want to modify an element ("author") of the meta of each document. 
# At the end, we have to return the modified element (the document with 
# the updated "author" meta).
corpus$content <- map2(.x = content(corpus), .y = good_data$Description,
        ~ {
            .x$meta[['author']] <- .y
            .x
          }
)

map(watch, ~ content(corpus[[.]]))
map(watch, ~ meta(corpus[[.]]))
```
<!----------------------------------------------------------------------------->




## Preprocesing

Before to create the Document-Term Matrix (DTM) of our corpus, we need
to perform on it some manipulation useful to assure the maximum
meaningfulness of its content.


### Lowercase

The first step is to remember that R is case sensitive and if we want to
analyze the words of several documents in terms of frequency or
importance or any other characteristics of them, we have to be sure that
every equal word is actually seen as equal. A common practice is to
lowercase all the words of each document.

```{r lowercase}
map(watch, ~ content(corpus[[.]]))

corpus <- tm_map(
    x   = corpus,
    FUN = content_transformer(tolower)
)

map(watch, ~ content(corpus[[.]]))
```


### Removing stopwords

Not meaningful words (generally this means "very common" but its not
true every time) shouldn't be considered to avoid unwanted noise in our
data.

```{r}
# stopwords('english')

corpus <- tm_map(corpus,
            removeWords, c(stopwords('english'),"approach","weak")
)

map(watch, ~ content(corpus[[.]]))
```


### Removing punctuation

Punctuation are (often) a source of great noise and redundancy (commonly
words are detected by programs by chunks of characters divided by white
space, e.g. every last word of a sentence is considered different from
its own, because it has a dot attached at the end).

```{r punctuation}
corpus <- tm_map(corpus, removePunctuation)

map(watch, ~ content(corpus[[.]]))
```


### Removing numbers

Generally numbers are not considered in basic text analyses

```{r numbers}
corpus <- tm_map(corpus, removeNumbers)

map(watch, ~ content(corpus[[.]]))
```


### Removing whitespace

All the "elimination" performed until now have left a great amount of
redundant white spaces.

```{r}
corpus <- tm_map(corpus, stripWhitespace)

map(watch, ~ content(corpus[[.]]))
```


### Stemming

To merge more than a word into a single token we can perform a stemming of the 
words, passing to a not elementary human-readable bag of words, but now it
contains tokens representing "concept".
(Consider to use lemmatization too. A not-so-simple to install program but well
documented and quite easy-to-use to perform lemmatization is TreeTagger,
interfaced in R by the `koRpus` package and its function `treetag()`.)

```{r}
corpus <- tm_map(corpus, stemDocument)

map(watch, ~ content(corpus[[.]]))
```

> NOTE: `stemDocument()` (`?stemDocument`) is not the only stemmer, take
        a look at `SnowballC` too (`citation('SnowballC')`, after installing).



### Building 2-grams

Until now we had modified or deleted words to reduce the amount of token
while increasing the amount of (useful) information retained by each
one. Now we start to do the opposite: some information are impossible to
retain in single words (i.g. negations), so it can be useful to consider
token that are "sequence" of n words consecutively , i.e. nGram. This
step introduce a huge amount of redundancy and possible noise and for
this main reason it is not suggested to use n > 3 (and even n = 3 is
proved to introduce more noise than the amount of information usable).

```{r}
BigramTokenizer <- function(x) {
        words(x) %>% 
        ngrams(2) %>% 
        map_chr(~ str_c(., collapse = ' '))

# NOTE: standard R
#    unlist(
#        lapply(ngrams(words(x), 2), paste, collapse = " "),
#        use.names = FALSE
#    )
}

# we whant to retain the unigramm too on the side of the bigram!!
UniBigramTokenizer <- function(x){
    c(words(x), BigramTokenizer(x))
}

#---------------
# check and test
# --------------
'today an hands-on about text mining' %>% 
    VectorSource %>%
    VCorpus %>%
    .[[1]] %>% 
    BigramTokenizer

'today an hands-on about text mining' %>% 
    VectorSource %>%
    VCorpus %>%
    .[[1]] %>% 
    UniBigramTokenizer
```

### DTM

We are now ready to create our DTM. the tm package provide some more
possibility of manipulation, as the selection of bounds for the number
of characters that are considered for a token to be considered, as well
as the number of document it has to be present.

```{r dtm}
dtm <- DocumentTermMatrix(corpus,
    control = list(
        wordLengths = c(4, Inf), 
        # bounds      = list(global = c(1, Inf)), 
        weighting   = function(x) weightTfIdf(x, normalize = TRUE),
        tokenize    = UniBigramTokenizer 
    )
)

dtm
inspect(dtm[watch, 1:10])
```


### Dimensions

#### Sizes (optional)
```{r sizes}
dim(dtm)

# Expected size (8 Byte each element for numbers, if they was all
# integers all stored `as.integer` it could be 4 Byte each one):
dim(dtm) %>%
    prod(8) %>%
    pryr:::print.bytes()

# Actual size:
pryr::object_size(dtm) # How is it possible???? Take a look of sparsity...

# NOTE: `pryr` is included in `tidyverse` but not directly loaded by it,
#        so we have to specify to R "were" to find the function we use

class(dtm)
str(dtm)

length(dtm$i) %>%
    prod(4) %>% # i, j, v, names
    sum(
        length(dtm$dimnames$Docs),
        length(dtm$dimnames$Terms)
    ) %>%
    prod(8) %>% 
    pryr:::print.bytes()
```


#### Quantity

We have obtained a very huge matrix difficult to manage and to use. 

```{r}
dim(dtm)
dim(removeSparseTerms(dtm, .2))
dim(removeSparseTerms(dtm, .97))
dim(removeSparseTerms(dtm, .99))
dim(removeSparseTerms(dtm, .999))
dim(removeSparseTerms(dtm, .9993))
dim(removeSparseTerms(dtm, .9996))
dim(removeSparseTerms(dtm, .9999))
#
dtm <- removeSparseTerms(dtm, .999)
dim(dtm)
```






## Analysis

We are now ready for the last part of the exercise: get some output.

### Frequencies

We call here frequencies , but remember we are using tf-idf weights and
not frequencies! So everything from here referred as "frequency" it
should be think as "importance".

Find all the terms more with a weight grater than 100.
```{r frequecies}
frequent <- findFreqTerms(
    x        = dtm,
    lowfreq  = 100,
    highfreq = Inf
)

frequent
```

Look at the redundancy of the component of the bi-gram...


### Associations

Find the token most associated with the most weighted ones

```{r}
findAssocs(
    x        = dtm,
    terms    = frequent,
    corlimit = 0.1
)
```






## Visualization

To draw plots it is useful to have an _ad hoc_ structure. So, lets
create a data frame with token and the relative weight, ordered from the
greatest to the lowest.

```{r, echo=TRUE}
word_frequencies <- dtm %>% 
    as.matrix %>% 
    colSums %>% 
    sort(decreasing = TRUE) %>% 
    data_frame(
        word   = names(.),
        weight = .
    )

word_frequencies
```


### Barplot

```{r, fig.fullwidth=TRUE, message=FALSE, warning=FALSE, echo=TRUE}
word_frequencies %>% 
    filter(weight > 70) %>% 
    ggplot(aes(x = word, y = weight)) + 
    geom_bar(stat = 'identity', aes(fill = weight)) + 
    theme(
        axis.text.x = element_text(angle = 45, hjust = 1)
    )
ggsave('word_frequencies.pdf')
```


### Wordcloud

And finally, the word cloud.

```{r cloud, fig.cap='Word cloud', message=FALSE, warning=FALSE, echo=TRUE}
wordcloud(
    words        = word_frequencies$word,
    freq         = word_frequencies$weight,
    min.freq     = 70,
    random.order = TRUE,
    rot.per      = .33,
    colors       = c('black', 'red', 'blue', 'yellow', 'green','magenta'),
    random.color = TRUE
)
```



### Network

Provided by `tm` there are a plot methods to draw the (weighted)
correlation network of the (subset selected into the) dtm. It is based
on a plot method provided by `Rgraphviz` which is no more into the CRAN.
You can install it by the bioconductor:

    source("http://bioconductor.org/biocLite.R")
        biocLite("Rgraphviz")


With default parameter is quite unuseful...

```{r, echo=TRUE}
plot(dtm #,
#     terms        = sample(Terms(x), 20),
#     corThreshold = 0.7,
#     weighting    = FALSE
)
```


But, you can easily obtain something more informative, like this

```{r, echo=TRUE}
# ?plot.TermDocumentMatrix
plot(dtm,
     terms        = findFreqTerms(dtm, lowfreq = 100),
     corThreshold = 0.05,        # Do not plot correlations below this threshold
     weighting    = FALSE    # whether line width corresponds to the correlation
)
```










Other possible way to draw a network


```{r net}
 # bring a subset of interest
 dtm_reduced <- dtm[, findFreqTerms(dtm, lowfreq = 100)]
 termMatrix <- t(as.matrix(dtm_reduced)) %*% (as.matrix(dtm_reduced))

 # inspect terms numbered 5 to 10
 termMatrix[5:10,5:10]

 # build a graph from the above matrix, removing loops
 g <- graph.adjacency(termMatrix, weighted = TRUE, mode = "undirected") %>% 
     igraph::simplify() # purrr and igraph share the same name for function
 
 # set labels and degrees of vertices (take a look at `?V`)
 V(g)$label  <- V(g)$name
 V(g)$degree <- degree(g)

 # plot the results your preferred layout
 plot(g, layout = layout.kamada.kawai)
# tkplot(g, layout = layout.kamada.kawai)

 # other options can be customized
 V(g)$label.color <- rgb(0, 0, .2, .8)
 V(g)$frame.color <- 'green'
 
 egam <- (((log(E(g)$weight) + .4) / max(log(E(g)$weight) + .4)) + 1)
 egam <- egam/max(egam)

 E(g)$color <- rgb(.5, .5, 0, egam)
 E(g)$width <- egam
 
 plot(g,   layout = layout.kamada.kawai)
# tkplot(g, layout = layout.kamada.kawai)
```


# Links 

+ R help: `?`
+ tm manual: <https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf>
+ tm example: <https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/>
+ TF-iDF: <http://www.r-bloggers.com/the-tf-idf-statistic-for-keyword-extraction/>
+ build a search engine:<http://anythingbutrbitrary.blogspot.it/2013/03/build-search-engine-in-20-minutes-or.html>
+ ggplot2 tutorial: <http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html>
+ bioconductor: <https://www.bioconductor.org>
