---
title: "Resampling -- Hands-on"
author: "Dott.ssa Ileana Baldi -- Dott. Corrado Lanera"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
---
**R package `caret` and resampling**

# Outline
+ caret overview
    - general info and scope
    - main functions and options
+ practical example
    - load and data managment
    - resampling strategies
    - train models each
    - model visualization and comparison
    
    
    
    
    
    
# `caret` overview

First of all we have to ensure the package is installed, and next loaded.
After that take a look at its `citation()`.

```{r, echo=TRUE}
if (!requireNamespace('caret')) install.packages('caret', dependencies = TRUE)

library('caret', quietly = TRUE)

citation('caret')
```



## General info and scope

In R there are huge amount of packages implementing functions to run different
or same kind of models.
There were __many different conventions for any one of those__.

+ `caret` provides __uniform framework__ to different models (147!)
    
    - linear discriminant analysis
    - regression
    - naive Bayes
    - support vector machines
    - classification and regression trees
    - random forests
    - boosting
    - ...
    
+ simplify model tuning using different kind of resampling
   
    - cv
    - repeatedcv
    - leave one out
    - leave group out
    - bootstrap
    - ...

+ simplify parameter tuning using same interface from different algorithms
    
    - (automatic or user-defined) grid search
    - (automatic) random search

+ (allow parallel processing)


## Main functions and options

+ (pre-processing    : `preProcess()`)
+ resampling
    - partitioning-cv : `createDataPartition()`
    - kFold-cv        : `createFolds()`
    - bootstrap       : `createResample()`
+ train model      : `train()`
+ (apply/test model : `predict()`)


```{r, eval=FALSE}
?createDataPartition 
```
    createDataPartition(y,                                  # vector of outcomes
        times  = 1,                             # number of partitions to create
        p      = 0.5,                 # percentage of data that goes to training
        list   = TRUE,             # should the results be a list? (or a matrix)
        groups = min(5, length(y))    # for numeric y, number of quantile breaks
    )


```{r, eval=FALSE}
?createFolds
```
    createFolds(y,
        k           = 10,                                      # number of folds
        list        = TRUE, 
        returnTrain = FALSE    # only if `list = TRUE`, train (or test) indexes?
    )


```{r, eval=FALSE}
?createResample
```
    function (y,
        times = 10,
        list  = TRUE
    )
    
    
```{r, eval=FALSE}
?train
```
    train(
        x,              # samples in rows and features in columns (admit sparse)
        y,                     # numeric or factor vector containing the outcome
        
        method     = "rf",     # which classification or regression model to use
        
        preProcess = NULL,  # string vector that defines a pre-processing of the
                            # predictor data. i.e. one or more of: "BoxCox",
                            # "YeoJohnson", "expoTrans", "center", "scale",
                            # "range", "knnImpute", "bagImpute", "medianImpute",
                            # "pca", "ica" and "spatialSign"
        
        ...,   # NOTE: dots are not the last! from now all args has to be named!
        
        weights    = NULL,                                        # case weights
        
        metric     = ifelse(is.factor(y), "Accuracy", "RMSE"), # what summary
                            # metric will be used to select the optimal model.
                            # i.e. "RMSE" and "Rsquared" for regression and
                            # "Accuracy" and "Kappa" for classification
        
        maximize   = ifelse(metric %in% c("RMSE", "logLoss"), FALSE, TRUE),
                            # should the metric be maximized (or minimized)?
        
        trControl  = trainControl(),                    # how this function acts
        tuneGrid   = NULL,            # a data frame with possible tuning values
                                      # SUGESTION: use `expand.grind()`
        tuneLength = 3     # amount of granularity in the tuning parameter grid.
                           # or maximum number of tuning parameter combinations
                           # that will be generated by the random search
    )





    train(form, data,
        ...,                                             # all the previous ones
        weights,
        subset,       # index vector for cases to be used in the training sample
        na.action = na.fail, # action to be taken if NAs are found (`na.fail`
                             # which fail if find a missing value, or `na.omit`
                             # which consider only samples with non-missing
                             # values)
        contrasts = NULL        # contrasts to be used for (some or all) factors
                                                # variables in the model formula
    )













# Practical example

We will see an example from a data base named __infarto2.csv__ which is a 
*comma-separated value* (of the second type) source of data, i.e. a text file in
which each row is an observation and each columns is a variable, identified by a
semicolon (;).

The file have nine variables:
- ATS     : [dichotomic] Arterial Tortuosity Syndrome, 
- SIG     : [integer]    Cigarettes per day;
- GLIC    : [integer]    Glycemic index;
- TRIG    : [integer]    Triglycerides;
- GRAL    : [numerical]  Alcohol Consumption (g) per day;
- ICAM    : [integer]    Inter-cellular Adhesion Molecule;
- VCAM    : [integer]    Vascular Adhesion Molecule;
- OMOCIST : [numerical]  Homocysteine;
- B6      : [numerical]  Vitamin B6.

We would like to import this dataset into R, and try different way to train a 
__k-Nearest Neighbour__ (kNN) machine by tuning the relevant parameters via
some validation procedure, and test it in a (considered) independent (sub)set.
The aims is to predict the value of *Intercellular Adhesion Molecule* (`ICAM`)
based on the *Homocysteine*'s one (`OMOCIST`)





## Load and data managment

First of all load the date which is supposed to be in the same folder of the
present file (or, anyway in the folder returned by `getwd()`).
```{r}
files <- list.files(getwd())
if (!'infarto2.csv' %in% files) {       # we try to take care of possible errors
    stop('You need "infarto2.csv" file into your working directory')
}

# R convert strings to factors by default (why? boh!), so we have to remember
# ALWAYS to include `stringAsFactors = FALSE` when import data or create a 
# data.frame
data_infarto <- read.csv2(file = 'infarto2.csv', stringsAsFactors = FALSE)

# some data quality check
dim(data_infarto)
names(data_infarto)
```




It is a good idea (imo) to lowercase all the variables names to avoid errors
in typing or multiple matching with similar but different names
(R is case-sensitive!)
```{r}
names(data_infarto) <- tolower(names(data_infarto))
names(data_infarto)
```




We know which predictors are of numerical, integer or factorial values.
Check and change them if necessary.
```{r}
str(data_infarto)
data_infarto[c(2:4, 6, 7)] <- lapply(data_infarto, as.integer)
                        # you CAN ignore this...
                        # but: this line of code work because a data.frame is
                        # actually a list and lapply return a list; so,
                        # `data_infarto[c(2:4, 6, 7)]` asks to
                        # R: "please, when you will assign me the result of the
                        # computation you are performing, try to not change my
                        # class (which is data.frame)", and because it is easy
                        # for R to consider a list as a data.frame, your
                        # request will be satisfied.
                        # NB: data_infarto <- lapply(...) will re-assign
                        # data_infarto to a list, if you want to change all
                        # the data.frame remember to perform
                        # `data_infarto[] <- lapply(...)`, i.e. empty brackets
                        # means "all"!).
data_infarto[c('gral', 'omocist', 'b6')] <- lapply(
    data_infarto[c('gral', 'omocist', 'b6')],
    as.numeric
)
data_infarto[[1]] <- as.factor(data_infarto[[1]])
data_infarto
```




Well now that we know how to import and perform (basic) management of data
we can select only the variables we are interested id

```{r}
data_used <- data_infarto[c('vcam', 'omocist')]
data_used
```




Lets check for missing values...
```{r}
vapply(data_infarto, function(var) sum(is.na(var)), FUN.VALUE = integer(1))
```

We are lucky and the vcam nor omocist have missing data... anyway, missing data
management is out of the scope of this Hands-On...





## Resampling strategies

+ Exhaustive __Cross-Validation__

     - __Leave one out__
```{r}
loo_train <- createFolds(data_used$vcam,
    k           = nrow(data_used),
    list        = TRUE,
    returnTrain = TRUE
)
lapply(loo_train, function(fold) fold[1:10])[1:10]

loo_test <- createFolds(data_used$vcam,
    k           = nrow(data_used),
    list        = TRUE,
    returnTrain = FALSE
)
loo_test[1:10]

loo_test_matrix <- createFolds(data_used$vcam,
    k           = nrow(data_used),
    list        = FALSE,
    returnTrain = FALSE
)
loo_test_matrix # warning: now it is actually a vector...
```





+
    - __leave p-out__

There are no function in care explicitly designed for that in the `caret`
package, probably for the obvious reason that no computer is able to manage that
huge amount of data if p is not quite all or a negligible portion of the
samples in a big data process (and it still true in any normal one!...)).
You could try to force `createDataPartition()`, which request us for a
percentage of data that goes to the training set, passing to it the "correct"
proportion to have exactly n - p data in the training set... but a PC is not an
exact calculator and error of approximation quite sure will brake the tricks
(and, in our case, anyway try to compute the number of subset of dimension 10 
there were in our data set...is it tricky to compute the result without 
an overflow error from the machine you are using, but still possible...
the results is 54,177,963,902,922,784. Please, try to train that number of
machine...)




+ Non-exhaustive __Cross-Validation__

    - __k-fold__

This is (quite) the same as before for the LOO procedure. Try a 5-Fold CV.
```{r}
kfive_train <- createFolds(data_used$vcam,
    k           = 5,
    list        = TRUE,
    returnTrain = TRUE
)
lapply(kfive_train, function(fold) fold[1:10])

kfive_test <- createFolds(data_used$vcam,
    k           = 5,
    list        = TRUE,
    returnTrain = FALSE
)
lapply(kfive_test, function(fold) fold[1:10])
test_sizes <- vapply(kfive_test, length, FUN.VALUE = integer(1))
test_sizes
sum(test_sizes)
```

+
    - __Repeated random sub-sampling__ (named "leave group out" in caret)
Here we can use, properly, the function `createDataPartition()`, e.g. asking 
for 100 sub-sampling with 0.1 proportion of data in the test side.

```{r}
sub_train <- createDataPartition(data_used$vcam,
    times       = 100,
    p           = 0.9,      # the function return the index for the training set
    list        = TRUE
)
lapply(sub_train, function(fold) fold[1:10])
```





+ __Bootstrap__

as expected we will see a simple function provided by the `caret` package for 
bootstrap sampling.

```{r}
bootstrap_train <- createResample(data_used$vcam,
    times = 100,
    list = TRUE
)

length(bootstrap_train)
length(bootstrap_train[[1]])
lapply(bootstrap_train, function(fold) fold[1:10])

different_bootstrap_values <- vapply(bootstrap_train,
       function(boot) length(unique(boot)) / nrow(data_used),
       FUN.VALUE = numeric(1)
)
mean(different_bootstrap_values)
```


## Train models each

```{r}
## validation parameters
k_fold      <- 5
repetitions <- 10
p           <- 0.1

## machine parameters, i.e. kNN
## <https://topepo.github.io/caret/available-models.html>
## ?kNN
k_nearest_searched <- seq(from = 1, to = 100, by = 5)
kernel_considered  <- c(
    "rectangular", "triangular", "cos", "inv", "gaussian", "rank", "optimal"
)
kind_of_distances  <- c(1, 2)                  # Manhattan (Taxi) and Euclideian

## search grid
parameter_grid_search <- expand.grid(
    kmax     = k_nearest_searched,
    kernel   = kernel_considered,
    distance = kind_of_distances
)                                 # 34 * 7 * 2 = 280 different machines each MLT
parameter_grid_search

## alternative (and automatic) tune parameters searching

# search     <- 30                                             # in trainControl
# tuneLength <- 'random'              # standard is "grid" which means: in order
```

```{r}
message('simple')
system.time(
    simple_model <- train(vcam ~ omocist,
            data       = data_used,
            method     = 'kknn',
    
            ## preProcess = NULL,
            ## weights    = NULL,
            ## metric     = 'RMSE',
            ## maximize   = FALSE
    
            trControl  = trainControl(method  = 'none'),
            tuneLength = 1            # default is 3,
                                      # in the simple case it has to be set to 1
    )
)




message('loo')
system.time(
    loo_model <- train(vcam ~ omocist,
            data       = data_used,
            method     = 'kknn',
            trControl  = trainControl(method  = 'LOOCV'),
            tuneGrid   = parameter_grid_search
    )
)






message('repeatedcv')
system.time(
    cv_model <- train(vcam ~ omocist,
            data       = data_used,
            method     = 'kknn',
    
            trControl  = trainControl(method  = 'repeatedcv',
                            number  = k_fold,     # 5
                            repeats = repetitions # 10
            ),
            tuneGrid   = parameter_grid_search
    )
)






message('lgo')
system.time(
    partition_model <- train(vcam ~ omocist,
            data       = data_used,
            method     = 'kknn',
    
            trControl  = trainControl(method  = 'LGOCV',
                            p       = 1 - p,      #  0.9
                            number  = repetitions # 10
            ),
            tuneGrid   = parameter_grid_search
    )
)






message('bootstrap')
system.time(
    boot_model <- train(vcam ~ omocist,
            data       = data_used,
            method     = 'kknn',
    
            trControl  = trainControl(method  = 'boot',
                            number  = repetitions # 10
            ),
            tuneGrid   = parameter_grid_search
    )
)
```









## Model visualization and comparison
```{r}
    plot(loo_model)
    plot(cv_model)
    plot(partition_model)
    plot(boot_model)
```


```{r}
model_list <- list(
    loo       = loo_model,
    cv        = cv_model,
    partition = partition_model,
    boot      = boot_model
)

model_list[[1]]$bestTune

bests <- vapply(model_list,
    function(model) model$bestTune,
    vector('list', 3)
)
bests[3, ] <- as.character(unlist(bests[3, ]))
bests


same_sampling <- model_list[c(3, 4)]
str(same_sampling, 1)
same_togheter <- resamples(same_sampling)

summary(same_togheter)

summary(diff(same_togheter))
```
